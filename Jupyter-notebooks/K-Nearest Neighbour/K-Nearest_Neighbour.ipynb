{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1098511,"sourceType":"datasetVersion","datasetId":614016}],"dockerImageVersionId":29867,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# k-Nearest Neighbors (kNN)\nThis Jupyter notebook summarizes the <a href=#pros>Pros</a> and <a href=#cons>Cons</a> of the k-Nearest Neighbors algorithm and gives two Python examples on usage for <a href=#clas>Classification</a> and <a href=#reg>Regression</a>. \n\n## Theory<sup>1,2,3</sup>  \n* Is a non-probabilistic, non-parametric and instance-based learning algorithm (see <a href=#reference>References</a>:\n    * **Non-parametric** means it makes no explicit assumptions about the function form of _h_, avoiding the dangers of mis-modelling the underlying distribution of the data\n        * For example, suppose our data is highly non-Gaussian but the learning model was choose assumes a Gaussian form. In that case, a parametric algorithm would make extremely poor predictions.\n    * **Instance-based** learning means that the algorithm does not explicitly learn a model\n        * Instead, it chooses to memorize the training instances which are subsequently used as \"knowledge\" for the prediction phase\n        * Concretely, this means that only when a query to our database is made (i.e., when we ask it to predict a label given an input), will the algorithm use the training instances to predict the result\n\n### Pros<a name=\"pros\"/> \n* **simple** to understand and implement\n* with **little to zero training time**\n* kNN **works just as easily with multi-class data** sets whereas other algorithms are hard-coded for the binary setting\n* the non-parametric nature of kNN gives it an edge in certain settings where the data may be highly unusual, thus **without prior knowledge on distribution**\n\n### Cons<a name=\"cons\"/> \n* **computationally expensive** testing phase\n    * we **need to store the whole data set for each decision**!\n* can **suffer from skewed class distributions**\n    * for example, if a certain class is very frequent in the training set, it will tend to dominate the majority voting of the new example (large number = more common)\n* the accuracy can be severally **degraded with high-dimension data** because of the little difference between the nearest and farthest neighbor\n    * **the curse of dimensionality** refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience\n    * for high-dimensional data (e.g., with number of dimensions more than 10) **scaling** and **dimension reductions** (such as PCA) is usually performed prior applying kNN\n    \n### References<a name=\"reference\"/>  \n* <sup>1</sup>Wikipedia [kNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm), [Curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) \n* <sup>2</sup>Sklearn [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n* <sup>3</sup>[Complete Guide to K-Nearest-Neighbors](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor)","metadata":{}},{"cell_type":"markdown","source":"## Classification<a name=\"clas\"/> \n* the output is a class membership\n* an object is classified by a **majority vote** of its neighbours, with the object being assigned to the class most common among its k nearest neighbours\n    * if k = 1, then the object is simply assigned to the class of that nearest neighbour\n    \n    \n### Example: predict [IRIS](https://scikit-learn.org/stable/datasets/index.html#iris-dataset) class","metadata":{}},{"cell_type":"markdown","source":"Set environment","metadata":{}},{"cell_type":"code","source":"# Scikit-learn\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\nfrom sklearn.dummy import DummyClassifier, DummyRegressor\nfrom sklearn.metrics import classification_report, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\n# other libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use vector drawing inside jupyter notebook\n%config InlineBackend.figure_format = \"svg\"\n# Set matplotlib default axis font size (inside this notebook)\nplt.rcParams.update({'font.size': 8})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load data","metadata":{}},{"cell_type":"code","source":"iris = datasets.load_iris()\n# This is extra step that can be omitted but Pandas DataFrame contains some powerfull features\ndf = pd.DataFrame(iris.data,columns=iris.feature_names)\ndf = df.assign(target=iris.target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show data summary: extend the `describe` method by selected stats\n* See the Jupyter notebook on **Standard Procedure** for more details","metadata":{}},{"cell_type":"code","source":"# Compute selected stats\ndfinfo = pd.DataFrame(df.dtypes,columns=[\"dtypes\"])\nfor (m,n) in zip([df.count(),df.isna().sum()],[\"count\",\"isna\"]):\n    dfinfo = dfinfo.merge(pd.DataFrame(m,columns=[n]),right_index=True,left_index=True,how=\"inner\");\n# Add to `describe` output\ndfinfo.T.append(df.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show histogram (distribution)","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(9,2))\nfor (i,v) in enumerate(df.columns):\n    plt.subplot(1,df.shape[1],i+1);\n    plt.hist(df.iloc[:,i],bins=\"sqrt\")\n    plt.title(df.columns[i],fontsize=9);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show correlation matrix","metadata":{}},{"cell_type":"code","source":"df.corr().round(2).style.background_gradient(cmap=\"viridis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scale and try to **reduce dimensions**: what we try to do is to **always simply the model** if possible (see correlation matrix above)\n* More complex model (e.g., more features, or higher _*k*_) will (in theory) increase the probability of higher \"out of sample\" error (even when \"in sample\" error = train set) will be smaller!\n* Use either 99% threshold (own subjective) or \"mle\" algorithm (more objective)\n* Use **linear** scaler (transformation)\n* Here, the data is scaled prior train-test split. \n    * In real applications, first split and scale afterwards, to simulate real-world scenario where we do not have the test set! (otherwise data snooping effect)","metadata":{}},{"cell_type":"code","source":"scale = StandardScaler(with_mean=True,with_std=True);\nXo = scale.fit_transform(df.drop([\"target\"],axis=1).values);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=0.99)# or set n_components=\"mle\"\nX = pca.fit_transform(Xo)\nprint(\"Nr. of features after PCA = {} (input = {})\".format(X.shape[1],Xo.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare for fitting","metadata":{}},{"cell_type":"code","source":"# encode target values (is not necessary for IRIS but still:-)\ny = LabelEncoder().fit_transform(df[\"target\"].values);\n# Split 2/3 to 1/3 train to test respectively\n[X_train,X_test,y_train,y_test] = train_test_split(X,y,train_size = 0.67,test_size = 0.33,\n                                                   stratify=y,random_state=123);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Find optimal model\n* Considering the small data set (150 samples), find \"optimal\" k setting it to maximum of 5\n    * Optimal in terms of accuracy\n    * Simple model = higher probability of lower in and out-of sample error","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier(algorithm=\"auto\");\nparameters = {\"n_neighbors\":[1,3,5],\n              \"weights\":[\"uniform\",\"distance\"]}\nmodel_optim = GridSearchCV(model, parameters, cv=5,scoring=\"accuracy\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_optim.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show the \"optimal\" settings for kNN","metadata":{}},{"cell_type":"code","source":"model_optim.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Show resulting accuracy","metadata":{}},{"cell_type":"code","source":"for (i,x,y) in zip([\"Train\",\"Test\"],[X_train,X_test],[y_train,y_test]):\n    print(\"Classification kNN\",i,\" report:\\n\",classification_report(y,model_optim.predict(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this case, the precision (accuracy=macro avg precision) is very high. \nJust to show that that is not coincidence compare to \"dummy\" model (most frequent & uniform distribution)","metadata":{}},{"cell_type":"code","source":"for i in [\"most_frequent\",\"uniform\"]:\n    dummy = DummyClassifier(strategy=i).fit(X_train,y_train);\n    print(\"Classification \",i,\" test report:\",classification_report(y_test,dummy.predict(X_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regression<a name=\"reg\"/> \n* Predicts value as the **average of the values** of its k nearest neighbors\n\n### Example: Predict House price\n* Use Scikit-learn [California Housing](https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset) data set\n    * This is a large data set that allows us to use more complex model\n* Nontheless, try to reduce the number of features: via visual inspection and using PCA","metadata":{}},{"cell_type":"markdown","source":"Load data","metadata":{}},{"cell_type":"code","source":"# house = datasets.fetch_california_housing()\n# df = pd.DataFrame(house.data,columns=house.feature_names)\n# df = df.assign(target=house.target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/test-data/california_housing.csv\").drop(columns=[\"Unnamed: 0\"],errors='ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inspect data: show statistics, histogram and correlation ","metadata":{}},{"cell_type":"code","source":"# Compute selected stats\ndfinfo = pd.DataFrame(df.dtypes,columns=[\"dtypes\"])\nfor (m,n) in zip([df.count(),df.isna().sum()],[\"count\",\"isna\"]):\n    dfinfo = dfinfo.merge(pd.DataFrame(m,columns=[n]),right_index=True,left_index=True,how=\"inner\");\n# Add to `describe` output\ndfinfo.T.append(df.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(9,4))\nfor (i,v) in enumerate(df.columns):\n    plt.subplot(2,5,i+1);\n    plt.hist(df.iloc[:,i],50,density=True)\n    plt.legend([df.columns[i]],fontsize=6);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr().round(2).style.background_gradient(cmap=\"viridis\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Supervised Reduction\n* Considering the correlation, histogram and the summary table:\n    * Remove/drop \"AveOccup\" (average house occupancy)","metadata":{}},{"cell_type":"code","source":"df = df.drop([\"AveOccup\"],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare for fitting by scaling data set\n* Here, the data is scaled prior train-test split. \n    * In real applications, first split and scale afterwards, to simulate real-world scenario where we do not have the test set!","metadata":{}},{"cell_type":"code","source":"X = StandardScaler().fit_transform(df.drop(\"target\",axis=1).values);\ny = df.target.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = PCA(n_components=\"mle\").fit_transform(X)\nprint(\"Nr. of features after reduction = {} (input = {})\".format(X.shape[1],df.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Fit model","metadata":{}},{"cell_type":"code","source":"[X_train,X_test,y_train,y_test] = train_test_split(X,y,train_size=0.67,test_size=0.33,\n                                                   random_state=123);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsRegressor();\nparameters = {\"n_neighbors\":[1,3,5,7,9],\n              \"weights\":[\"uniform\",\"distance\"]}\nknn_reg = GridSearchCV(knn, parameters, cv=5, scoring=\"neg_mean_squared_error\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_reg.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_reg.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Regression kNN (test) RMSE \\t= {:.0f} *1000$\".format(\n    100*np.sqrt(mean_squared_error(knn_reg.predict(X_test),y_test))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare to dummy","metadata":{}},{"cell_type":"code","source":"for i in [\"mean\",\"median\"]:\n    dummy = DummyRegressor(strategy=i).fit(X_train,y_train);\n    print(\"Regression \",i,\"(test) RMSE \\t= {:.0f} *1000$\".format(\n        100*np.sqrt(mean_squared_error(y_test,dummy.predict(X_test)))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}